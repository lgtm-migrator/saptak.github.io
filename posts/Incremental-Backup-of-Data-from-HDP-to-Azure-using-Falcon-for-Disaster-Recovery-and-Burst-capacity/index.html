	
	<!doctype html>
<html lang="en">
  <head>    
    <title>saptak.in - Incremental Backup of Data from HDP to Azure using Falcon for Disaster Recovery and Burst capacity</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">

    
    <link href="/css/milk.min.css" rel="stylesheet">
    <link href="/css/milk-responsive.min.css" rel="stylesheet">     
    <link href="/css/style.css" rel="stylesheet" type="text/css" media="all">
    <link href="/css/fonts.css" rel="stylesheet" type="text/css" media="all">

    <script src='https://cdn.firebase.com/js/client/2.0.4/firebase.js'></script>
    <link rel="shortcut icon" href="/images/alexfinn.ico"> 
    <link rel="apple-touch-icon" href="">
    <link rel="canonical" href="http://saptak.github.io/posts/Incremental-Backup-of-Data-from-HDP-to-Azure-using-Falcon-for-Disaster-Recovery-and-Burst-capacity/">

    
    <link href="/rss.xml" type="application/atom+xml" rel="alternate" title="saptak.github.io">    

  </head>
  <body>    
    <div class="navbar navbar-fixed-top">        
  <div id="navbar-inner">
          <div id="logo">
            <a href="http://saptak.in"><img src="/images/letter-a.png" width="100px"></img></a>
          </div>
  </div>
</div>

<div class="container">
  <div class="content">
    <div class="row-fluid">
      <div class="span12">
        <div class="posts">
      

	    
	  <div class="post">
	    <header class="post-header">
	        <h1><a href="/posts/Incremental-Backup-of-Data-from-HDP-to-Azure-using-Falcon-for-Disaster-Recovery-and-Burst-capacity/">Incremental Backup of Data from HDP to Azure using Falcon for Disaster Recovery and Burst capacity</a></h1>
	        <div class="post-time">February 8 2015</div>
	    </header>
	    <div class="post-after">
	        <div class="tags">
	            
	        </div>
	    </div>
	    <hr>
	    <div class="post content">
	        

<h2 id="introduction:3b4d68d2b2e7a23ada5b6252c859a493">Introduction</h2>

<p>Apache Falcon simplifies the configuration of data motion with: replication; lifecycle management; lineage and traceability. This provides data governance consistency across Hadoop components.</p>

<h2 id="scenario:3b4d68d2b2e7a23ada5b6252c859a493">Scenario</h2>

<p>In this tutorial we will walk through a scenario where email data gets processed on multiple HDP 2.2 clusters around the country then gets backed up hourly on a cloud hosted cluster . In our example:</p>

<ul>
<li>This cluster is hosted on Windows Azure.</li>
<li>Data arrives from all the West Coast production servers. The input data feeds are often late for up to 4 hrs.</li>
</ul>

<p>The goal is to clean the raw data to remove sensitive information like credit card numbers and make it available to our marketing data science team for customer churn analysis.</p>

<p>To simulate this scenario, we have a pig script grabbing the freely available Enron emails from the internet and feeding it into the pipeline.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/tutorial/falcon/images/arch.png" alt="" />
</p>

<h2 id="prerequisite:3b4d68d2b2e7a23ada5b6252c859a493">Prerequisite</h2>

<ul>
<li>A cluster with Apache Hadoop 2.2 configured</li>
<li>A cluster with Apache Falcon configured</li>
</ul>

<p>The easiest way to meet the above prerequisites is to download the <a href="http://hortonworks.com/downloads">HDP Sandbox</a></p>

<p>After downloading the environment, confirm that Apache Falcon is running. Below are the steps to validate that:</p>

<ol>
<li>if Ambari is not configured on your Sandbox, go <code>http://127.0.0.1:8000/about/</code> and enable Ambari.</li>
</ol>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image01.png" alt="" />
</p>

<ol>
<li>Once Ambari is enabled, navigate to Ambari at <code>http://127.0.0.1:8080</code>, login with username and password of<code>admin</code> and <code>admin</code> respectively. Then check if Falcon is running.</li>
</ol>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image04.png" alt="" />
</p>

<ol>
<li>If Falcon is not running, start Falcon:</li>
</ol>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image07.png" alt="" />
</p>

<h2 id="steps-for-the-scenario:3b4d68d2b2e7a23ada5b6252c859a493">Steps for the Scenario</h2>

<ol>
<li>Create cluster specification XML file</li>
<li>Create feed (aka dataset) specification XML file

<ul>
<li>Reference cluster specification</li>
</ul></li>
<li>Create the process specification XML file

<ul>
<li>Reference cluster specification – defines where the process runs</li>
<li>Reference feed specification – defines the datasets that the process manipulates</li>
</ul></li>
</ol>

<p>We have already created the necessary xml files. In this step we are going to download the specifications and use them to define the topology and submit the storm job.</p>

<h3 id="staging-the-component-of-the-app-on-hdfs:3b4d68d2b2e7a23ada5b6252c859a493">Staging the component of the App on HDFS</h3>

<p>In this step we will stage the pig script and the necessary folder structure for inbound and outbound feeds on the HDFS:</p>

<p>First download this <a href="http://hortonassets.s3.amazonaws.com/tutorial/falcon/falcon.zip">zip file</a> called <a href="http://hortonassets.s3.amazonaws.com/tutorial/falcon/falcon.zip"><code>falcon.zip</code></a> to your local host machine.</p>

<p>Navigate using your browser to the Hue – File Browser interface at <a href="http://127.0.0.1:8000/filebrowser/">http://127.0.0.1:8000/filebrowser/</a> to explore the HDFS.</p>

<p>Navigate to <code>/user/ambari-qa</code> folder like below:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/tutorial/falcon/images/file-browser.png" alt="" />
</p>

<p>Now we will upload the zip file we just downloaded:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/tutorial/falcon/images/uploadzip.png" alt="" />
</p>

<p>This should also unzip the zip file and create a folder structure with a folder called <code>falcon</code>.</p>

<h3 id="setting-up-the-destination-storage-on-microsoft-azure:3b4d68d2b2e7a23ada5b6252c859a493">Setting up the destination storage on Microsoft Azure</h3>

<p>Login to the Windows Azure portal at <a href="http://manage.windowsazure.com/">http://manage.windowsazure.com</a></p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image13.png" alt="" />
</p>

<p>Create a storage account</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image16.png" alt="" />
</p>

<p>Wait for the storage account to be provisioned</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image19.png" alt="" />
</p>

<p>Copy the access key and the account name in a text document. We will use the access key and the account name in later steps</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image22.png" alt="" />
</p>

<p>The other information you will want to note down is the blob endpoint of the storage account we just created</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image25.png" alt="" />
</p>

<p>Click on the <code>Containers</code> tab and create a new container called <code>myfirstcontainer</code>.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image26.png" alt="" />
</p>

<h3 id="configuring-access-to-azure-blob-store-from-hadoop:3b4d68d2b2e7a23ada5b6252c859a493">Configuring access to Azure Blob store from Hadoop</h3>

<p>Login to Ambari – <a href="http://127.0.0.1:8080">http://127.0.0.1:8080</a> with the credentials <code>admin</code> and <code>admin</code>.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image27.png" alt="" />
</p>

<p>Then click on HDFS from the bar on the left and then select the <code>Configs</code> tab.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image28.png" alt="" />
</p>

<p>Scroll down to the bottom of the page to the <code>Custom hdfs-site</code> section and click on <code>Add property...</code></p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image31.png" alt="" />
</p>

<p>In the <code>Add Property</code> dialog, the key name will start with <code>fs.azure.account.key.</code> followed by your blob endpoint that you noted down in a previous step. The value will be the Azure storage key that you noted down in a previous step. Once you have filled in the values click the <code>Add</code> button:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image34.png" alt="" />
</p>

<p>Once you are back out of the new key dialog you will have to <code>Save</code> it by clicking on the green <code>Save</code> button:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image31.png" alt="" />
</p>

<p>Then restart all the service by clicking on the orange <code>Restart</code> button:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image36.png" alt="" />
</p>

<p>Wait for all the restart to complete</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image38.png" alt="" />
</p>

<p>Now let’s test if we can access our container on the Azure Blob Store.</p>

<p>SSH in to the VM:</p>

<p><code>ssh root@127.0.0.1 -p 2222;</code></p>

<p>The password is <code>hadoop</code></p>

<p><code>hdfs dfs -ls -R wasb://myfirstcontainer@saptak.blob.core.windows.net/</code></p>

<p>Issue the command from our cluster on the SSH’d terminal</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image40.png" alt="" />
</p>

<h3 id="staging-the-specifications:3b4d68d2b2e7a23ada5b6252c859a493">Staging the specifications</h3>

<p>From the SSH session, first we will change our user to <code>ambari-qa</code>. Type:</p>

<p><code>su ambari-qa</code></p>

<p>Go to the users home directory:</p>

<p><code>cd ~</code></p>

<p>Download the topology, feed and process definitions:</p>

<p><code>wget http://hortonassets.s3.amazonaws.com/tutorial/falcon/falconDemo.zip</code></p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image10.png" alt="" />
</p>

<p>Unzip the file:</p>

<p><code>unzip ./falconDemo.zip</code></p>

<p>Change Directory to the folder created:</p>

<p><code>cd falconChurnDemo/</code></p>

<p>Now let’s modify the <code>cleansedEmailFeed.xml</code> to point the backup cluster to our Azure Blob Store container.</p>

<p>Use <code>vi</code> to edit the file:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image42.png" alt="" />
</p>

<p>Modify the value of <code>location</code> element of the <code>backupCluster</code></p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image44.png" alt="" />
</p>

<p>to look like this:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image46.png" alt="" />
</p>

<p>Then save it and quit vi.</p>

	    </div>
	    
	<div class="about">
	<p> 
     
    </p>
</div>
		<nav id="pagination">
			<a class="prev" href="http://saptak.github.io/about/">Prev</a>
			<a class="next" href="http://saptak.github.io/posts/Real-time-Data-Ingestion-in-HBase-Hive-using-Storm-Bolt/">Next</a>
		</nav>
	
		        <footer>
		        	Powered by <a href="https://github.com/saptak">GitHub</a> 
		        	<p>© Saptak Sen 2015</p>
		        </footer>
		    </div>
		  </div>    
		</div>
      </div>
    </div>
</body>

<script type="text/javascript">
  var _paq = _paq || [];
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var u=(("https:" == document.location.protocol) ? "https" : "http") + ":change-me";
    _paq.push(['setTrackerUrl', u+'piwik.php']);
    _paq.push(['setSiteId', 4]);
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0]; g.type='text/javascript';
    g.defer=true; g.async=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s);
  })();
</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-59342910-1', 'auto');
  ga('send', 'pageview');

</script>
<noscript><p><img src="http://change-me" style="border:0;" alt="" /></p></noscript>


</html>
