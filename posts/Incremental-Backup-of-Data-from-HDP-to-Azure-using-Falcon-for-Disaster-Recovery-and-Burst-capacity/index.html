	
	<!doctype html>
<html lang="en">
  <head>    
    <title>saptak.in - Incremental Backup of Data from HDP to Azure using Falcon for Disaster Recovery and Burst capacity</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">

    
    <link href="/css/milk.min.css" rel="stylesheet">
    <link href="/css/milk-responsive.min.css" rel="stylesheet">     
    <link href="/css/style.css" rel="stylesheet" type="text/css" media="all">
    <link href="/css/fonts.css" rel="stylesheet" type="text/css" media="all">

	<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/solarized_dark.css">
	<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.pack.js"></script>
	<script>hljs.initHighlightingOnLoad();</script>
    <script src='https://cdn.firebase.com/js/client/2.0.4/firebase.js'></script>
    <link rel="shortcut icon" href="/images/alexfinn.ico"> 
    <link rel="apple-touch-icon" href="">
    <link rel="canonical" href="http://saptak.github.io/posts/Incremental-Backup-of-Data-from-HDP-to-Azure-using-Falcon-for-Disaster-Recovery-and-Burst-capacity/">

    
    <link href="/rss.xml" type="application/atom+xml" rel="alternate" title="saptak.github.io">    

  </head>
  <body>    
    <div class="navbar navbar-fixed-top">        
  <div id="navbar-inner">
          <div id="logo">
            <a href="http://saptak.in"><img src="/images/letter-a.png" width="100px"></img></a>
          </div>
  </div>
</div>

<div class="container">
  <div class="content">
    <div class="row-fluid">
      <div class="span12">
        <div class="posts">
      

	    
	  <div class="post">
	    <header class="post-header">
	        <h1><a href="/posts/Incremental-Backup-of-Data-from-HDP-to-Azure-using-Falcon-for-Disaster-Recovery-and-Burst-capacity/">Incremental Backup of Data from HDP to Azure using Falcon for Disaster Recovery and Burst capacity</a></h1>
	        <div class="post-time">February 8 2015</div>
	    </header>
	    <div class="post-after">
	        <div class="tags">
	            
	        </div>
	    </div>
	    <hr>
	    <div class="post content">
	        

<h2 id="introduction:3b4d68d2b2e7a23ada5b6252c859a493">Introduction</h2>

<p>Apache Falcon simplifies the configuration of data motion with: replication; lifecycle management; lineage and traceability. This provides data governance consistency across Hadoop components.</p>

<h2 id="scenario:3b4d68d2b2e7a23ada5b6252c859a493">Scenario</h2>

<p>In this tutorial we will walk through a scenario where email data gets processed on multiple HDP 2.2 clusters around the country then gets backed up hourly on a cloud hosted cluster . In our example:</p>

<ul>
<li>This cluster is hosted on Windows Azure.</li>
<li>Data arrives from all the West Coast production servers. The input data feeds are often late for up to 4 hrs.</li>
</ul>

<p>The goal is to clean the raw data to remove sensitive information like credit card numbers and make it available to our marketing data science team for customer churn analysis.</p>

<p>To simulate this scenario, we have a pig script grabbing the freely available Enron emails from the internet and feeding it into the pipeline.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/tutorial/falcon/images/arch.png" alt="" />
</p>

<h2 id="prerequisite:3b4d68d2b2e7a23ada5b6252c859a493">Prerequisite</h2>

<ul>
<li>A cluster with Apache Hadoop 2.2 configured</li>
<li>A cluster with Apache Falcon configured</li>
</ul>

<p>The easiest way to meet the above prerequisites is to download the <a href="http://hortonworks.com/downloads">HDP Sandbox</a></p>

<p>After downloading the environment, confirm that Apache Falcon is running. Below are the steps to validate that:</p>

<ol>
<li>if Ambari is not configured on your Sandbox, go <code>http://127.0.0.1:8000/about/</code> and enable Ambari.</li>
</ol>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image01.png" alt="" />
</p>

<ol>
<li>Once Ambari is enabled, navigate to Ambari at <code>http://127.0.0.1:8080</code>, login with username and password of<code>admin</code> and <code>admin</code> respectively. Then check if Falcon is running.</li>
</ol>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image04.png" alt="" />
</p>

<ol>
<li>If Falcon is not running, start Falcon:</li>
</ol>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image07.png" alt="" />
</p>

<h2 id="steps-for-the-scenario:3b4d68d2b2e7a23ada5b6252c859a493">Steps for the Scenario</h2>

<ol>
<li>Create cluster specification XML file</li>
<li>Create feed (aka dataset) specification XML file

<ul>
<li>Reference cluster specification</li>
</ul></li>
<li>Create the process specification XML file

<ul>
<li>Reference cluster specification – defines where the process runs</li>
<li>Reference feed specification – defines the datasets that the process manipulates</li>
</ul></li>
</ol>

<p>We have already created the necessary xml files. In this step we are going to download the specifications and use them to define the topology and submit the storm job.</p>

<h3 id="staging-the-component-of-the-app-on-hdfs:3b4d68d2b2e7a23ada5b6252c859a493">Staging the component of the App on HDFS</h3>

<p>In this step we will stage the pig script and the necessary folder structure for inbound and outbound feeds on the HDFS:</p>

<p>First download this <a href="http://hortonassets.s3.amazonaws.com/tutorial/falcon/falcon.zip">zip file</a> called <a href="http://hortonassets.s3.amazonaws.com/tutorial/falcon/falcon.zip"><code>falcon.zip</code></a> to your local host machine.</p>

<p>Navigate using your browser to the Hue – File Browser interface at <a href="http://127.0.0.1:8000/filebrowser/">http://127.0.0.1:8000/filebrowser/</a> to explore the HDFS.</p>

<p>Navigate to <code>/user/ambari-qa</code> folder like below:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/tutorial/falcon/images/file-browser.png" alt="" />
</p>

<p>Now we will upload the zip file we just downloaded:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/tutorial/falcon/images/uploadzip.png" alt="" />
</p>

<p>This should also unzip the zip file and create a folder structure with a folder called <code>falcon</code>.</p>

<h3 id="setting-up-the-destination-storage-on-microsoft-azure:3b4d68d2b2e7a23ada5b6252c859a493">Setting up the destination storage on Microsoft Azure</h3>

<p>Login to the Windows Azure portal at <a href="http://manage.windowsazure.com/">http://manage.windowsazure.com</a></p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image13.png" alt="" />
</p>

<p>Create a storage account</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image16.png" alt="" />
</p>

<p>Wait for the storage account to be provisioned</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image19.png" alt="" />
</p>

<p>Copy the access key and the account name in a text document. We will use the access key and the account name in later steps</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image22.png" alt="" />
</p>

<p>The other information you will want to note down is the blob endpoint of the storage account we just created</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image25.png" alt="" />
</p>

<p>Click on the <code>Containers</code> tab and create a new container called <code>myfirstcontainer</code>.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image26.png" alt="" />
</p>

<h3 id="configuring-access-to-azure-blob-store-from-hadoop:3b4d68d2b2e7a23ada5b6252c859a493">Configuring access to Azure Blob store from Hadoop</h3>

<p>Login to Ambari – <a href="http://127.0.0.1:8080">http://127.0.0.1:8080</a> with the credentials <code>admin</code> and <code>admin</code>.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image27.png" alt="" />
</p>

<p>Then click on HDFS from the bar on the left and then select the <code>Configs</code> tab.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image28.png" alt="" />
</p>

<p>Scroll down to the bottom of the page to the <code>Custom hdfs-site</code> section and click on <code>Add property...</code></p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image31.png" alt="" />
</p>

<p>In the <code>Add Property</code> dialog, the key name will start with <code>fs.azure.account.key.</code> followed by your blob endpoint that you noted down in a previous step. The value will be the Azure storage key that you noted down in a previous step. Once you have filled in the values click the <code>Add</code> button:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image34.png" alt="" />
</p>

<p>Once you are back out of the new key dialog you will have to <code>Save</code> it by clicking on the green <code>Save</code> button:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image31.png" alt="" />
</p>

<p>Then restart all the service by clicking on the orange <code>Restart</code> button:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image36.png" alt="" />
</p>

<p>Wait for all the restart to complete</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image38.png" alt="" />
</p>

<p>Now let’s test if we can access our container on the Azure Blob Store.</p>

<p>SSH in to the VM:</p>

<p><code>ssh root@127.0.0.1 -p 2222;</code></p>

<p>The password is <code>hadoop</code></p>

<p><code>hdfs dfs -ls -R wasb://myfirstcontainer@saptak.blob.core.windows.net/</code></p>

<p>Issue the command from our cluster on the SSH’d terminal</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image40.png" alt="" />
</p>

<h3 id="staging-the-specifications:3b4d68d2b2e7a23ada5b6252c859a493">Staging the specifications</h3>

<p>From the SSH session, first we will change our user to <code>ambari-qa</code>. Type:</p>

<p><code>su ambari-qa</code></p>

<p>Go to the users home directory:</p>

<p><code>cd ~</code></p>

<p>Download the topology, feed and process definitions:</p>

<p><code>wget http://hortonassets.s3.amazonaws.com/tutorial/falcon/falconDemo.zip</code></p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image10.png" alt="" />
</p>

<p>Unzip the file:</p>

<p><code>unzip ./falconDemo.zip</code></p>

<p>Change Directory to the folder created:</p>

<p><code>cd falconChurnDemo/</code></p>

<p>Now let’s modify the <code>cleansedEmailFeed.xml</code> to point the backup cluster to our Azure Blob Store container.</p>

<p>Use <code>vi</code> to edit the file:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image42.png" alt="" />
</p>

<p>Modify the value of <code>location</code> element of the <code>backupCluster</code></p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image44.png" alt="" />
</p>

<p>to look like this:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/falcon2/image46.png" alt="" />
</p>

<p>Then save it and quit vi.</p>

<h3 id="submit-the-entities-to-the-cluster:3b4d68d2b2e7a23ada5b6252c859a493">Submit the entities to the cluster:</h3>

<h4 id="cluster-specification:3b4d68d2b2e7a23ada5b6252c859a493">Cluster Specification</h4>

<p>Cluster specification is one per cluster.</p>

<p>See below for a sample cluster specification file.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/tutorial/falcon/images/cluster-spec.png" alt="" />
</p>

<p>Back to our scenario, lets submit the ‘oregon cluster’ entity to Falcon. This signifies the primary Hadoop cluster located in the Oregon data center.</p>

<p><code>falcon entity -type cluster -submit -file oregonCluster.xml</code></p>

<p>Then lets submit the ‘virginia cluster’ entity to Falcon. This signifies the backup Hadoop cluster located in the Virginia data center</p>

<p><code>falcon entity -type cluster -submit -file virginiaCluster.xml</code></p>

<p>If you view the XML file you will see how the cluster location and purpose has been captured in the XML file.</p>

<h4 id="feed-specification:3b4d68d2b2e7a23ada5b6252c859a493">Feed Specification</h4>

<p>A feed (a.k.a dataset) signifies a location of data and its associated replication policy and late arrival cut-off time.</p>

<p>See below for a sample feed (a.k.a dataset) specification file.</p>

<p><img src="http://hortonassets.s3.amazonaws.com/tutorial/falcon/images/feed-spec.png" alt="" />
</p>

<p>Back to our scenario, let’s submit the source of the raw email feed. This feed signifies the raw emails that are being downloaded into the Hadoop cluster. These emails will be used by the email cleansing process.</p>

<p><code>falcon entity -type feed -submit -file rawEmailFeed.xml</code></p>

<p>Now let’s define the feed entity which will handle the end of the pipeline to store the cleansed email. This feed signifies the emails produced by the cleanse email process. It also takes care of replicating the cleansed email dataset to the backup cluster (virginia cluster)</p>

<p><code>falcon entity -type feed -submit -file cleansedEmailFeed.xml</code></p>

<h4 id="process:3b4d68d2b2e7a23ada5b6252c859a493">Process</h4>

<p>A process defines configuration for a workflow. A workflow is a directed acyclic graph(DAG) which defines the job for the workflow engine. A process definition defines the configurations required to run the workflow job. For example, process defines the frequency at which the workflow should run, the clusters on which the workflow should run, the inputs and outputs for the workflow, how the workflow failures should be handled, how the late inputs should be handled and so on.</p>

<p>Here is an example of what a process specification looks like:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/tutorial/falcon/images/process-spec.png" alt="" />
</p>

<p>Back to our scenario, let’s submit the ingest and the cleanse process respectively:</p>

<p>The ingest process is responsible for calling the Oozie workflow that downloads the raw emails from the web into the primary Hadoop cluster under the location specified in the rawEmailFeed.xml It also takes care of handling late data arrivals</p>

<p><code>falcon entity -type process -submit -file emailIngestProcess.xml</code></p>

<p>The cleanse process is responsible for calling the pig script that cleans the raw emails and produces the clean emails that are then replicated to the backup Hadoop cluster</p>

<p><code>falcon entity -type process -submit -file cleanseEmailProcess.xml</code></p>

<h3 id="schedule-the-falcon-entities:3b4d68d2b2e7a23ada5b6252c859a493">Schedule the Falcon entities</h3>

<p>So, all that is left now is to schedule the feeds and processes to get it going.</p>

<h4 id="ingest-the-feed:3b4d68d2b2e7a23ada5b6252c859a493">Ingest the feed</h4>

<p><code>falcon entity -type feed -schedule -name rawEmailFeed</code></p>

<p><code>falcon entity -type process -schedule -name rawEmailIngestProcess</code></p>

<h4 id="cleanse-the-emails:3b4d68d2b2e7a23ada5b6252c859a493">Cleanse the emails</h4>

<p><code>falcon entity -type feed -schedule -name cleansedEmailFeed</code></p>

<p><code>falcon entity -type process -schedule -name cleanseEmailProcess</code></p>

<h3 id="processing:3b4d68d2b2e7a23ada5b6252c859a493">Processing</h3>

<p>In a few seconds you should notice that that Falcon has started ingesting files from the internet and dumping them to new folders like below on HDFS:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/tutorial/falcon/images/input.png" alt="" />
</p>

<p>In a couple of minutes you should notice a new folder called processed under which the files processed through the data pipeline are being emitted:</p>

<p><img src="http://hortonassets.s3.amazonaws.com/tutorial/falcon/images/output.png" alt="" />
</p>

<p>We just created an end-to-end data pipeline to process data. The power of the Apache Falcon framework is its flexibility to work with pretty much any open source or proprietary data processing products out there.</p>

	    </div>
	    
	<div class="about">
	<p> 
     
    </p>
</div>
		<nav id="pagination">
			<a class="prev" href="http://saptak.github.io/about/">Prev</a>
			<a class="next" href="http://saptak.github.io/posts/Real-time-Data-Ingestion-in-HBase-Hive-using-Storm-Bolt/">Next</a>
		</nav>
	
		        <footer>
		        	Powered by <a href="https://github.com/saptak">GitHub</a> 
		        	<p>© Saptak Sen 2015</p>
		        </footer>
		    </div>
		  </div>    
		</div>
      </div>
    </div>
</body>

<script type="text/javascript">
  var _paq = _paq || [];
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var u=(("https:" == document.location.protocol) ? "https" : "http") + ":change-me";
    _paq.push(['setTrackerUrl', u+'piwik.php']);
    _paq.push(['setSiteId', 4]);
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0]; g.type='text/javascript';
    g.defer=true; g.async=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s);
  })();
</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-59342910-1', 'auto');
  ga('send', 'pageview');

</script>
<noscript><p><img src="http://change-me" style="border:0;" alt="" /></p></noscript>


</html>
